{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaboraDev/hackathon_one_flightOnTime/blob/main/pipeline_ds1_ds2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPCMoTTD3x17"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Semana 2 - DS1 (Pré-processamento) e DS2 (Feature Engineering)\n",
        "--------------------------------------------------------------\n",
        "\n",
        "Objetivo:\n",
        "- DS1: criar um pipeline de pré-processamento unificado\n",
        "        (limpeza, encoding, normalização) reutilizável.\n",
        "- DS2: criar features temporais e features agregadas de atraso\n",
        "        em um transformer próprio, também reutilizável.\n",
        "\n",
        "Ao final, salvamos:\n",
        "- media_transformer_ds2.pkl  -> transformer de médias de atraso (DS2)\n",
        "- preprocessor_ds1.pkl       -> pipeline de pré-processamento (DS1)\n",
        "- X_train_processed.npy      -> dados prontos para modelagem\n",
        "- y_train.npy                -> alvo (0 = pontual, 1 = atraso > 15 min)\n",
        "\"\"\"\n",
        "\n",
        "# ========================\n",
        "# 1. Imports principais\n",
        "# ========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aF8cT7m3z8F"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# 2. Funções para carregar e preparar os dados (DS1)\n",
        "# =======================================================\n",
        "\n",
        "def baixar_e_extrair_dados(zip_path: str, extract_folder: str, file_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Baixa o arquivo .zip do Google Drive (se ainda não existir)\n",
        "    e extrai o conteúdo para uma pasta.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    zip_path : str\n",
        "        Caminho onde o arquivo .zip será salvo.\n",
        "    extract_folder : str\n",
        "        Pasta onde os arquivos serão extraídos.\n",
        "    file_id : str\n",
        "        ID do arquivo no Google Drive.\n",
        "    \"\"\"\n",
        "    import gdown  # import local para evitar erro se não for usar em outro ambiente\n",
        "\n",
        "    if not os.path.exists(zip_path):\n",
        "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "        gdown.download(url, zip_path, quiet=False)\n",
        "\n",
        "    if not os.path.exists(extract_folder):\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(extract_folder)\n",
        "\n",
        "\n",
        "def carregar_vra(pasta: str,\n",
        "                 padrao: str = \"VRA_*.csv\",\n",
        "                 sep: str = \";\",\n",
        "                 encoding: str = \"latin-1\",\n",
        "                 skiprows: int = 1) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Carrega todos os arquivos VRA_*.csv de uma pasta e concatena\n",
        "    em um único DataFrame.\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    DataFrame com todos os voos concatenados.\n",
        "    \"\"\"\n",
        "    caminho_busca = os.path.join(pasta, padrao)\n",
        "    arquivos = sorted(glob.glob(caminho_busca))\n",
        "\n",
        "    if not arquivos:\n",
        "        raise FileNotFoundError(f\"Nenhum arquivo encontrado em {caminho_busca}\")\n",
        "\n",
        "    dfs = []\n",
        "    for arquivo in arquivos:\n",
        "        df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
        "        dfs.append(df_temp)\n",
        "\n",
        "    df_final = pd.concat(dfs, ignore_index=True)\n",
        "    return df_final\n",
        "\n",
        "\n",
        "def renomear_colunas(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Padroniza nomes das colunas para snake_case em português simples.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Garante que acentos sejam lidos corretamente\n",
        "    df.columns = [c.encode(\"latin1\").decode(\"utf-8\") for c in df.columns]\n",
        "\n",
        "    mapa_colunas = {\n",
        "        \"ICAO Empresa Aérea\": \"empresa_aerea\",\n",
        "        \"Número Voo\": \"numero_voo\",\n",
        "        \"Código Autorização (DI)\": \"codigo_autorizacao_di\",\n",
        "        \"Código Tipo Linha\": \"codigo_tipo_linha\",\n",
        "        \"ICAO Aeródromo Origem\": \"aerodromo_origem\",\n",
        "        \"ICAO Aeródromo Destino\": \"aerodromo_destino\",\n",
        "        \"Partida Prevista\": \"partida_prevista\",\n",
        "        \"Partida Real\": \"partida_real\",\n",
        "        \"Chegada Prevista\": \"chegada_prevista\",\n",
        "        \"Chegada Real\": \"chegada_real\",\n",
        "        \"Situação Voo\": \"situacao_voo\",\n",
        "        \"Código Justificativa\": \"codigo_justificativa\",\n",
        "    }\n",
        "\n",
        "    df = df.rename(columns=mapa_colunas)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzGCwymZ32LG"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# 3. Qualidade dos dados e criação do alvo (DS1)\n",
        "# =======================================================\n",
        "\n",
        "def criar_flags_qualidade(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria flags de qualidade para datas inválidas e períodos extremos.\n",
        "\n",
        "    - Converte 'partida_prevista' e 'partida_real' para datetime.\n",
        "    - Marca linhas com datas fora do período esperado.\n",
        "    - Marca voos com diferença de horário muito alta (> 24h).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    df[\"partida_prevista\"] = pd.to_datetime(\n",
        "        df[\"partida_prevista\"],\n",
        "        format=\"%Y-%m-%d %H:%M:%S\",\n",
        "        errors=\"coerce\",\n",
        "    )\n",
        "    df[\"partida_real\"] = pd.to_datetime(\n",
        "        df[\"partida_real\"],\n",
        "        format=\"%Y-%m-%d %H:%M:%S\",\n",
        "        errors=\"coerce\",\n",
        "    )\n",
        "\n",
        "    df[\"flag_partida_prevista_ausente\"] = df[\"partida_prevista\"].isna()\n",
        "    df[\"flag_partida_real_ausente\"] = df[\"partida_real\"].isna()\n",
        "    df[\"flag_aerodromo_origem_ausente\"] = df[\"aerodromo_origem\"].isna()\n",
        "\n",
        "    df[\"flag_data_partida_fora_periodo\"] = (\n",
        "        df[\"partida_prevista\"].notna()\n",
        "        & (\n",
        "            (df[\"partida_prevista\"].dt.year < 2021)\n",
        "            | (df[\"partida_prevista\"].dt.year > 2025)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    limite_horas = 24\n",
        "    delta_h = (df[\"partida_real\"] - df[\"partida_prevista\"]).dt.total_seconds() / 3600\n",
        "    df[\"flag_partida_muito_alto\"] = (\n",
        "        df[\"partida_prevista\"].notna()\n",
        "        & df[\"partida_real\"].notna()\n",
        "        & (delta_h.abs() > limite_horas)\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def criar_target_atrasado(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria a coluna 'atrasado' (0 = pontual, 1 = atraso > 15 minutos).\n",
        "\n",
        "    - Calcula 'atraso_partida_min' (diferença entre partida_real e partida_prevista).\n",
        "    - Filtra linhas inválidas com base nas flags de qualidade.\n",
        "    - Cria a coluna binária 'atrasado'.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    df[\"atraso_partida_min\"] = (\n",
        "        (df[\"partida_real\"] - df[\"partida_prevista\"]).dt.total_seconds() / 60\n",
        "    )\n",
        "\n",
        "    filtros_validos = (\n",
        "        ~df[\"flag_partida_prevista_ausente\"]\n",
        "        & ~df[\"flag_partida_real_ausente\"]\n",
        "        & ~df[\"flag_data_partida_fora_periodo\"]\n",
        "        & ~df[\"flag_partida_muito_alto\"]\n",
        "    )\n",
        "\n",
        "    df = df[filtros_validos].copy()\n",
        "    df[\"atrasado\"] = (df[\"atraso_partida_min\"] > 15).astype(int)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc2biY4O34y8"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# 4. Amostragem estratificada (DS1)\n",
        "# =======================================================\n",
        "\n",
        "def criar_amostra_estratificada(\n",
        "    df: pd.DataFrame,\n",
        "    coluna_target: str = \"atrasado\",\n",
        "    frac: float = 0.1,\n",
        "    random_state: int = 42,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cria uma amostra estratificada simples, mantendo a proporção de atrasos.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    frac : float\n",
        "        Fração desejada (ex: 0.1 = 10% da base).\n",
        "    \"\"\"\n",
        "    df_amostra, _ = train_test_split(\n",
        "        df,\n",
        "        test_size=1 - frac,\n",
        "        random_state=random_state,\n",
        "        stratify=df[coluna_target],\n",
        "    )\n",
        "    return df_amostra\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7b03Jqa373a"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# 5. Features temporais (DS2)\n",
        "# =======================================================\n",
        "\n",
        "def criar_features_temporais(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"hora_dia\"] = df[\"partida_prevista\"].dt.hour\n",
        "    df[\"dia_semana\"] = df[\"partida_prevista\"].dt.dayofweek\n",
        "    df[\"mes_ano\"] = df[\"partida_prevista\"].dt.month\n",
        "\n",
        "    def classificar_periodo(hora: int) -> str:\n",
        "        if 5 <= hora < 12: return \"Manha\"\n",
        "        if 12 <= hora < 18: return \"Tarde\"\n",
        "        if 18 <= hora < 22: return \"Noite\"\n",
        "        return \"Madrugada\"\n",
        "\n",
        "    df[\"periodo_dia\"] = df[\"hora_dia\"].apply(classificar_periodo)\n",
        "    df[\"fim_de_semana\"] = df[\"dia_semana\"].isin([4, 5, 6]).astype(int)\n",
        "    df[\"alta_temporada\"] = df[\"mes_ano\"].isin([7, 12]).astype(int)\n",
        "\n",
        "    # --- NOVO (Requisito DS2): Transformações Log e Caps ---\n",
        "    df[\"atraso_log\"] = np.log1p(np.maximum(df[\"atraso_partida_min\"], 0))\n",
        "    df[\"atraso_capped\"] = np.clip(df[\"atraso_partida_min\"], 0, 120)\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P5QCkai3-Pw"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# 6. Transformer de médias de atraso (DS2)\n",
        "# =======================================================\n",
        "\n",
        "class MediaAtrasoTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Cria três features numéricas com médias de atraso (em minutos):\n",
        "\n",
        "    - media_atraso_empresa\n",
        "    - media_atraso_origem\n",
        "    - media_atraso_destino\n",
        "\n",
        "    Importante:\n",
        "    - Usa 'atraso_partida_min' apenas no treino (fit).\n",
        "    - Na produção, o transformer reutiliza as médias aprendidas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.medias_empresa = {}\n",
        "        self.medias_origem = {}\n",
        "        self.medias_destino = {}\n",
        "        self.media_global = 0.0\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y=None):\n",
        "        # Calcula médias por empresa, origem e destino\n",
        "        self.medias_empresa = (\n",
        "            X.groupby(\"empresa_aerea\")[\"atraso_partida_min\"].mean().to_dict()\n",
        "        )\n",
        "        self.medias_origem = (\n",
        "            X.groupby(\"aerodromo_origem\")[\"atraso_partida_min\"].mean().to_dict()\n",
        "        )\n",
        "        self.medias_destino = (\n",
        "            X.groupby(\"aerodromo_destino\")[\"atraso_partida_min\"].mean().to_dict()\n",
        "        )\n",
        "        self.media_global = float(X[\"atraso_partida_min\"].mean())\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
        "        X = X.copy()\n",
        "\n",
        "        X[\"media_atraso_empresa\"] = (\n",
        "            X[\"empresa_aerea\"].map(self.medias_empresa).fillna(self.media_global)\n",
        "        )\n",
        "        X[\"media_atraso_origem\"] = (\n",
        "            X[\"aerodromo_origem\"].map(self.medias_origem).fillna(self.media_global)\n",
        "        )\n",
        "        X[\"media_atraso_destino\"] = (\n",
        "            X[\"aerodromo_destino\"].map(self.medias_destino).fillna(self.media_global)\n",
        "        )\n",
        "\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4mzjDA_4BDE"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "# 7. Definição de features e pipeline de pré-processamento\n",
        "# =======================================================\n",
        "\n",
        "# Nome da coluna alvo\n",
        "TARGET_COL = \"atrasado\"\n",
        "\n",
        "# Features originais (categorias de companhia/rota/tipo de linha)\n",
        "FEATURES_ORIGINAIS = [\n",
        "    \"empresa_aerea\",\n",
        "    \"aerodromo_origem\",\n",
        "    \"aerodromo_destino\",\n",
        "    \"codigo_tipo_linha\",\n",
        "]\n",
        "\n",
        "# Features numéricas derivadas de tempo\n",
        "NUMERIC_FEATURES_BASE = [\n",
        "    \"hora_dia\",\n",
        "    \"dia_semana\",\n",
        "    \"mes_ano\",\n",
        "    \"fim_de_semana\",\n",
        "    \"alta_temporada\",\n",
        "]\n",
        "\n",
        "# Features categóricas finais\n",
        "CATEGORICAL_FEATURES = [\n",
        "    \"empresa_aerea\",\n",
        "    \"aerodromo_origem\",\n",
        "    \"aerodromo_destino\",\n",
        "    \"codigo_tipo_linha\",\n",
        "    \"periodo_dia\",\n",
        "]\n",
        "\n",
        "\n",
        "def montar_pipeline_preprocessamento():\n",
        "    \"\"\"\n",
        "    Cria o pipeline de pré-processamento com:\n",
        "    - Imputação de nulos para numéricas (mediana) + StandardScaler\n",
        "    - Imputação de nulos para categóricas (DESCONHECIDO) + OrdinalEncoder\n",
        "    \"\"\"\n",
        "    numeric_features_final = NUMERIC_FEATURES_BASE + [\n",
        "        \"media_atraso_empresa\",\n",
        "        \"media_atraso_origem\",\n",
        "        \"media_atraso_destino\",\n",
        "    ]\n",
        "\n",
        "    numeric_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scaler\", StandardScaler()),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    categorical_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\n",
        "                \"imputer\",\n",
        "                SimpleImputer(strategy=\"constant\", fill_value=\"DESCONHECIDO\"),\n",
        "            ),\n",
        "            (\n",
        "                \"encoder\",\n",
        "                OrdinalEncoder(\n",
        "                    handle_unknown=\"use_encoded_value\",\n",
        "                    unknown_value=-1,\n",
        "                ),\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_pipeline, numeric_features_final),\n",
        "            (\"cat\", categorical_pipeline, CATEGORICAL_FEATURES),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "    )\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "import json\n",
        "\n",
        "def realizar_analise_impacto(X_proc, y_val, nomes_colunas):\n",
        "    \"\"\"Gera o relatório de impacto das features (DS2).\"\"\"\n",
        "    df_temp = pd.DataFrame(X_proc, columns=nomes_colunas)\n",
        "    correlacao = df_temp.corrwith(pd.Series(y_val)).abs().sort_values(ascending=False)\n",
        "    print(\"\\n--- TOP 5 FEATURES POR IMPACTO ---\")\n",
        "    print(correlacao.head(5))\n",
        "\n",
        "def salvar_documentacao_json(arquivo=\"documentacao_ds1_ds2.json\"):\n",
        "    \"\"\"Cria o registro das features para o time de integração.\"\"\"\n",
        "    dados_doc = {\n",
        "        \"features\": [\"hora_dia\", \"mes_ano\", \"atraso_log\", \"atraso_capped\", \"media_atraso_empresa\"],\n",
        "        \"target\": \"atrasado (1 para atraso > 15min)\",\n",
        "        \"responsaveis\": \"Equipe DS1 e DS2\"\n",
        "    }\n",
        "    with open(arquivo, \"w\") as f:\n",
        "        json.dump(dados_doc, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew2upMFW4C8x",
        "outputId": "71d9dbaf-ae23-4d13-e6b4-0fbeb524a01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n",
            "/tmp/ipython-input-1969404820.py:51: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(arquivo, sep=sep, encoding=encoding, skiprows=skiprows)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TOP 5 FEATURES POR IMPACTO ---\n",
            "media_atraso_origem     0.130024\n",
            "media_atraso_empresa    0.127043\n",
            "codigo_tipo_linha       0.111690\n",
            "media_atraso_destino    0.086280\n",
            "hora_dia                0.077201\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# =======================================================\n",
        "# 8. Fluxo principal: carregar, transformar e salvar (DS1+DS2)\n",
        "# =======================================================\n",
        "\n",
        "# 8.1. Caminhos e parâmetros básicos\n",
        "FILE_ID = \"1207psedBKvnS0pJkDITroSzPiWrcz0ag\"  # mesmo ID do notebook original\n",
        "ZIP_PATH = \"dados_vra.zip\"\n",
        "EXTRACT_FOLDER = \"dados_vra\"\n",
        "PASTA_VRA = os.path.join(EXTRACT_FOLDER, \"dados_vra\")\n",
        "\n",
        "# 8.2. Carregar e preparar base\n",
        "baixar_e_extrair_dados(ZIP_PATH, EXTRACT_FOLDER, FILE_ID);\n",
        "\n",
        "df_raw = carregar_vra(pasta=PASTA_VRA)\n",
        "df_raw = renomear_colunas(df_raw)\n",
        "df_raw = criar_flags_qualidade(df_raw)\n",
        "df_raw = criar_target_atrasado(df_raw)\n",
        "\n",
        "# 8.3. Amostra estratificada para evitar uso de memória excessiva\n",
        "df_sample = criar_amostra_estratificada(df_raw, coluna_target=TARGET_COL, frac=0.1)\n",
        "\n",
        "# 8.4. Criar features temporais (DS2)\n",
        "df_sample = criar_features_temporais(df_sample)\n",
        "# df_sample = criar_feature_atraso_transformado(df_sample ) # Removido, pois a função não existe e as features já são criadas em criar_features_temporais\n",
        "\n",
        "# 8.5. Aplicar transformer de médias (DS2)\n",
        "media_transformer = MediaAtrasoTransformer()\n",
        "df_sample = media_transformer.fit_transform(df_sample)\n",
        "\n",
        "# 8.6. Separar X (features) e y (target)\n",
        "colunas_features = (\n",
        "    NUMERIC_FEATURES_BASE\n",
        "    + CATEGORICAL_FEATURES\n",
        "    + [\"media_atraso_empresa\", \"media_atraso_origem\", \"media_atraso_destino\"]\n",
        ")\n",
        "\n",
        "X = df_sample[colunas_features].copy()\n",
        "y = df_sample[TARGET_COL].copy()\n",
        "\n",
        "# 8.7. Montar e aplicar pipeline de pré-processamento (DS1)\n",
        "preprocessor = montar_pipeline_preprocessamento()\n",
        "X_processed = preprocessor.fit_transform(df_sample)\n",
        "\n",
        "nomes_f = NUMERIC_FEATURES_BASE + [\"media_atraso_empresa\", \"media_atraso_origem\", \"media_atraso_destino\"] + CATEGORICAL_FEATURES\n",
        "realizar_analise_impacto(X_processed, y.values, nomes_f)\n",
        "salvar_documentacao_json() # Gera o documentacao_ds1_ds2.json\n",
        "\n",
        "# 8.8. Salvar artefatos principais\n",
        "with open(\"media_transformer_ds2.pkl\", \"wb\") as f:\n",
        "    pickle.dump(media_transformer, f)\n",
        "\n",
        "with open(\"preprocessor_ds1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "\n",
        "np.save(\"X_train_processed.npy\", X_processed)\n",
        "np.save(\"y_train.npy\", y.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YGulxkV4Ofx",
        "outputId": "8adbd131-e7ee-4724-f53f-adcd89572e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "RESUMO FINAL - DS1 (Pré-processamento) e DS2 (Feature Engineering)\n",
            "================================================================================\n",
            "\n",
            "DS1 - Pré-processamento unificado:\n",
            "- Carregamento e concatenação dos arquivos VRA_*.csv\n",
            "- Renomeação de colunas para nomes padronizados\n",
            "- Criação de flags de qualidade e filtro de linhas inválidas\n",
            "- Criação da coluna alvo 'atrasado' (0 = pontual, 1 = atraso > 15 min)\n",
            "- Amostra estratificada de ~10% da base para reduzir uso de memória\n",
            "- Pipeline de pré-processamento com:\n",
            "  * Imputação de nulos (numéricas: mediana, categóricas: 'DESCONHECIDO')\n",
            "  * Normalização (StandardScaler) para numéricas\n",
            "  * OrdinalEncoder para categóricas\n",
            "\n",
            "DS2 - Feature Engineering:\n",
            "- Criação de features temporais a partir de 'partida_prevista':\n",
            "  * hora_dia, dia_semana, mes_ano, periodo_dia, fim_de_semana, alta_temporada\n",
            "- Implementação do MediaAtrasoTransformer, que cria:\n",
            "  * media_atraso_empresa, media_atraso_origem, media_atraso_destino\n",
            "- Todas as features são compatíveis com uso posterior em API\n",
            "\n",
            "Arquivos gerados:\n",
            "- media_transformer_ds2.pkl\n",
            "- preprocessor_ds1.pkl\n",
            "- X_train_processed.npy\n",
            "- y_train.npy\n"
          ]
        }
      ],
      "source": [
        "# =======================================================\n",
        "# 9. Resumo final do que foi feito (print)\n",
        "# =======================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RESUMO FINAL - DS1 (Pré-processamento) e DS2 (Feature Engineering)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nDS1 - Pré-processamento unificado:\")\n",
        "print(\"- Carregamento e concatenação dos arquivos VRA_*.csv\")\n",
        "print(\"- Renomeação de colunas para nomes padronizados\")\n",
        "print(\"- Criação de flags de qualidade e filtro de linhas inválidas\")\n",
        "print(\"- Criação da coluna alvo 'atrasado' (0 = pontual, 1 = atraso > 15 min)\")\n",
        "print(\"- Amostra estratificada de ~10% da base para reduzir uso de memória\")\n",
        "print(\"- Pipeline de pré-processamento com:\")\n",
        "print(\"  * Imputação de nulos (numéricas: mediana, categóricas: 'DESCONHECIDO')\")\n",
        "print(\"  * Normalização (StandardScaler) para numéricas\")\n",
        "print(\"  * OrdinalEncoder para categóricas\")\n",
        "\n",
        "print(\"\\nDS2 - Feature Engineering:\")\n",
        "print(\"- Criação de features temporais a partir de 'partida_prevista':\")\n",
        "print(\"  * hora_dia, dia_semana, mes_ano, periodo_dia, fim_de_semana, alta_temporada\")\n",
        "print(\"- Implementação do MediaAtrasoTransformer, que cria:\")\n",
        "print(\"  * media_atraso_empresa, media_atraso_origem, media_atraso_destino\")\n",
        "print(\"- Todas as features são compatíveis com uso posterior em API\")\n",
        "\n",
        "print(\"\\nArquivos gerados:\")\n",
        "print(\"- media_transformer_ds2.pkl\")\n",
        "print(\"- preprocessor_ds1.pkl\")\n",
        "print(\"- X_train_processed.npy\")\n",
        "print(\"- y_train.npy\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
